{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01ca6486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.13\n",
      "macOS: ('13.6', ('', '', ''), 'arm64')\n",
      "Machine: arm64\n",
      "MPS Built: True\n",
      "MPS Available: True\n",
      "Torch Version: 2.2.2\n"
     ]
    }
   ],
   "source": [
    "# testing out issues with mps\n",
    "import torch\n",
    "import platform\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"macOS:\", platform.mac_ver())\n",
    "print(\"Machine:\", platform.machine())\n",
    "print(\"MPS Built:\", torch.backends.mps.is_built())\n",
    "print(\"MPS Available:\", torch.backends.mps.is_available())\n",
    "print(\"Torch Version:\", torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62a4490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping invalid JSON line.\n",
      "Skipping invalid JSON line.\n",
      "Skipping invalid JSON line.\n",
      "Warning: No data loaded from JSON. Please check your file.\n"
     ]
    }
   ],
   "source": [
    "#loading packages and our data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "       \n",
    "\n",
    "# Load the data\n",
    "# bring in lastfm data\n",
    "import json\n",
    "\n",
    "parada= pd.json_normalize([json.loads(line) for line in open('data/parada_full_dataset.json')])\n",
    "parada.groupby(\"genre\").size().reset_index(name='counts').sort_values(by='counts', ascending=False)\n",
    "parada = parada.drop(parada[parada['genre'] != 'pop'].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "120d8258",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parada' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# now LIWC, NRC, distilbert all together : time for Factor Analysis\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mfactor_analyzer\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfactor_analyzer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m calculate_bartlett_sphericity\n\u001b[0;32m----> 4\u001b[0m parada_num \u001b[38;5;241m=\u001b[39m \u001b[43mparada\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumber\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# NEED TO LOOK AT THIS RE  NA VALUES \u001b[39;00m\n\u001b[1;32m      7\u001b[0m parada_num_clean \u001b[38;5;241m=\u001b[39m parada_num\u001b[38;5;241m.\u001b[39mdropna() \n",
      "\u001b[0;31mNameError\u001b[0m: name 'parada' is not defined"
     ]
    }
   ],
   "source": [
    "# now LIWC, NRC, distilbert all together : time for Factor Analysis\n",
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "\n",
    "parada_num = parada.select_dtypes(include='number')\n",
    "\n",
    "# NEED TO LOOK AT THIS RE  NA VALUES \n",
    "parada_num_clean = parada_num.dropna() \n",
    "print(f\"Rows before: {parada_num.shape[0]}, after dropna: {parada_num_clean.shape[0]}\")\n",
    "parada_num_clean = parada_num_clean.loc[:, parada_num_clean.std() != 0]\n",
    "print((parada_num_clean.std() == 0).sum()) \n",
    "\n",
    "\n",
    "chi_square_value,p_value=calculate_bartlett_sphericity(parada_num_clean)\n",
    "chi_square_value, p_value\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html\n",
    "# https://www.datacamp.com/tutorial/introduction-factor-analysis\n",
    "\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(parada_num_clean)\n",
    "print(\"kmo: \", kmo_model) # nan--issues\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46228dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# too many columns and not ready for factor analysis\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Remove columns with zero or near-zero variance\n",
    "selector = VarianceThreshold(threshold=1e-5)\n",
    "filtered_data = selector.fit_transform(parada_num_clean)\n",
    "\n",
    "# Optionally get remaining column names\n",
    "remaining_cols = parada_num_clean.columns[selector.get_support()]\n",
    "parada_num_filtered = pd.DataFrame(filtered_data, columns=remaining_cols)\n",
    "parada_num_filtered.drop(\"Segment\", inplace=True, axis=1, errors='ignore')\n",
    "\n",
    "#note had to edit error = sp. in factor analyzer.py to error = np. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e676d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify low-loading factors to remove\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "\n",
    "# Perform Factor Analysis\n",
    "fa = FactorAnalyzer(n_factors=2, rotation='varimax')\n",
    "fa.fit(parada_num_filtered)\n",
    "\n",
    "# Get factor loadings\n",
    "loadings = fa.loadings_\n",
    "\n",
    "# Set a threshold for removing low loadings\n",
    "threshold = 0.5\n",
    "\n",
    "# Identify columns (items) with low factor loadings\n",
    "low_loading_items = [parada_num_clean.columns[i] for i in range(loadings.shape[0]) if all(abs(loadings[i, :]) < threshold)]\n",
    "\n",
    "print(f\"Items with low factor loadings (below {threshold}):\")\n",
    "print(low_loading_items)\n",
    "\n",
    "# Remove those items from the original DataFrame\n",
    "parada_num_concise = parada_num_clean.drop(columns=low_loading_items)\n",
    "\n",
    "# Check the cleaned dataset\n",
    "print(f\"DataFrame after removing low loading items: {parada_num_concise.shape}\")\n",
    "\n",
    "print(f\"Remaining columns after removing low loading items: {parada_num_concise.columns.tolist()}\", f\"Num of remaining columns: {parada_num_concise.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5035ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Set threshold and number of factors\n",
    "threshold = 0.25\n",
    "n_factors = 2\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "parada_scaled = scaler.fit_transform(parada_num_filtered)\n",
    "\n",
    "# Convert back to DataFrame to keep column names\n",
    "parada_scaled_df = pd.DataFrame(parada_scaled, columns=parada_num_filtered.columns)\n",
    "\n",
    "# Work on a copy for iterative elimination\n",
    "parada_num_concise = parada_scaled_df.copy()\n",
    "\n",
    "for round_num in range(1, 8):  # Run 7 rounds\n",
    "    print(f\"\\n--- Round {round_num} ---\")\n",
    "\n",
    "    # Fit Factor Analysis\n",
    "    fa = FactorAnalyzer(n_factors=n_factors, rotation='varimax')\n",
    "    fa.fit(parada_num_concise)\n",
    "\n",
    "    # Get loadings\n",
    "    loadings = fa.loadings_\n",
    "\n",
    "    # Identify low-loading items\n",
    "    low_loading_items = [\n",
    "        parada_num_concise.columns[i]\n",
    "        for i in range(loadings.shape[0])\n",
    "        if all(abs(loadings[i, :]) < threshold)\n",
    "    ]\n",
    "\n",
    "    print(f\"Low-loading items (below {threshold}): {low_loading_items}\")\n",
    "\n",
    "    # Drop low-loading items\n",
    "    parada_num_concise.drop(columns=low_loading_items, inplace=True)\n",
    "\n",
    "    print( f\"Num of remaining columns: {parada_num_concise.shape[1]}\", f\"Remaining columns after removing low loading items: {parada_num_concise.columns.tolist()}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012fe751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the number of factors\n",
    "# \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fit FactorAnalyzer with max possible factors\n",
    "fa = FactorAnalyzer(n_factors=parada_num_concise.shape[1], rotation='varimax')\n",
    "fa.fit(parada_num_concise)\n",
    "\n",
    "# Get eigenvalues and plot them\n",
    "ev, _ = fa.get_eigenvalues()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(1, len(ev)+1), ev, marker='o', linestyle='-')\n",
    "plt.axhline(1, color='red', linestyle='--', label='Eigenvalue = 1')\n",
    "plt.title('Scree Plot for Factor Analysis')\n",
    "plt.xlabel('Number of Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca6b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for Factor Analysis on the smaller set of columns\n",
    "# redo our calculations:#\n",
    "parada_num_concise = parada_num_clean[['features.entropy', 'affective.NRCExpandedEmotion.Anger', 'features.readability.flesch_reading_ease', \n",
    "                                       'affective.LIWC.total_word_count', 'affective.LIWC.ppron']].dropna()\n",
    "#print(parada_num_concise.corr())\n",
    "parada_num_concise = parada_num_concise.dropna()\n",
    "#parada_num_concise.drop(\"Segment\", axis=1, inplace=True)\n",
    "\n",
    "#does it do better than identity matrix?\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(parada_num_concise)\n",
    "print(\"Bartlett’s Test: p =\", p_value, \"Chi-square =\", chi_square_value)\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html\n",
    "# https://www.datacamp.com/tutorial/introduction-factor-analysis\n",
    "\n",
    "#KMO calculations\n",
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all,kmo_model=calculate_kmo(parada_num_concise)\n",
    "kmo_model # 0.6 is not very good, 0.7 is acceptable, 0.8 is good, 0.9 is great\n",
    "# https://bookdown.org/luguben/EFA_in_R/kaiser-meyer-olkin-kmo.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5f3304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # determine the number of factors\n",
    "# # \n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Fit FactorAnalyzer with max possible factors\n",
    "# fa = FactorAnalyzer(n_factors=parada_num_concise.shape[1], rotation='varimax')\n",
    "# fa.fit(parada_num_concise)\n",
    "\n",
    "# # Get eigenvalues and plot them\n",
    "# ev, _ = fa.get_eigenvalues()\n",
    "\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(range(1, len(ev)+1), ev, marker='o', linestyle='-')\n",
    "# plt.axhline(1, color='red', linestyle='--', label='Eigenvalue = 1')\n",
    "# plt.title('Scree Plot for Factor Analysis')\n",
    "# plt.xlabel('Number of Factors')\n",
    "# plt.ylabel('Eigenvalue')\n",
    "# plt.legend()\n",
    "# plt.grid(True)\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e527078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready for factor analysis:\n",
    "# redo but limit to 2 factors\n",
    "fa = FactorAnalyzer(n_factors=2, rotation='varimax') \n",
    "fa.fit(parada_num_concise)\n",
    "\n",
    "# Get the variance explained\n",
    "variance, proportion_variance, cumulative_variance = fa.get_factor_variance()\n",
    "\n",
    "# Print variance results\n",
    "print(\"Variance per factor:\", variance)\n",
    "print(\"Proportion of variance per factor:\", proportion_variance)\n",
    "print(\"Cumulative variance (proportion):\", cumulative_variance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91671ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at loadings\n",
    "import seaborn as sns\n",
    "\n",
    "loadings = fa.loadings_\n",
    "\n",
    "# Create a DataFrame for factor loadings for easier visualization\n",
    "loadings_df = pd.DataFrame(loadings, columns=[f'Factor {i+1}' for i in range(loadings.shape[1])], index=parada_num_concise.columns)\n",
    "sorted_loadings_df = loadings_df.sort_values(by='Factor 2', ascending=False)\n",
    "\n",
    "\n",
    "# Print factor loadings for each variable\n",
    "print([sorted_loadings_df.head(10)])\n",
    "\n",
    "# Plot bar chart of loadings for each variable in each factor\n",
    "plt.figure(figsize=(10, 6))\n",
    "# Set the axis limits for zooming in\n",
    "sorted_loadings_df.plot(kind='bar', figsize=(12, 8))\n",
    "plt.title(\"Factor Loadings for Each Variable\")\n",
    "plt.xlabel(\"Variables\")\n",
    "plt.ylabel(\"Factor Loadings\")\n",
    "plt.xticks(rotation=90)  # Rotate variable names for readability\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358e30ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look more at loadings and results: biplot\n",
    "# biplot\n",
    "import matplotlib.pyplot as plt\n",
    "factor_scores = fa.transform(parada_num_concise)\n",
    "\n",
    "# Create a biplot: Factor scores (observations) and factor loadings (variables)\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot factor scores\n",
    "sns.scatterplot(x=factor_scores[:, 0], y=factor_scores[:, 1], alpha=0.6, label='Observations')\n",
    "\n",
    "# Plot factor loadings (arrows for variables)\n",
    "for i, var in enumerate(loadings_df.index):\n",
    "    plt.arrow(0, 0, loadings[i, 0], loadings[i, 1], color='red', alpha=0.7, head_width=0.05)\n",
    "    plt.text(loadings[i, 0] * 1.1, loadings[i, 1] * 1.1, var, color='black', ha='center', va='center', fontweight='bold')\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel('Factor 1')\n",
    "plt.ylabel('Factor 2')\n",
    "# Set the axis limits for zooming in\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-2, 2)\n",
    "plt.title('Biplot of Factor Scores and Factor Loadings')\n",
    "plt.axhline(0, color='gray',linewidth=0.5)\n",
    "plt.axvline(0, color='gray',linewidth=0.5)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a3077",
   "metadata": {},
   "source": [
    "## K-Means: clustering of factor results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3069da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Range of cluster sizes to evaluate\n",
    "k_range = range(2, 18)\n",
    "\n",
    "# Store inertia and silhouette scores\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "# Fit KMeans for each k and record metrics\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=13)\n",
    "    kmeans.fit(factor_scores)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "    labels = kmeans.labels_\n",
    "    sil_score = silhouette_score(factor_scores, labels)\n",
    "    silhouette_scores.append(sil_score)\n",
    "\n",
    "# Plot both inertia and silhouette score\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Inertia plot\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia', color='tab:blue')\n",
    "ax1.plot(k_range, inertias, marker='o', label='Inertia', color='tab:blue')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "# Silhouette score plot (secondary axis)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.set_ylabel('Silhouette Score', color='tab:green')\n",
    "ax2.plot(k_range, silhouette_scores, marker='s', linestyle='--', label='Silhouette Score', color='tab:green')\n",
    "ax2.tick_params(axis='y', labelcolor='tab:green')\n",
    "\n",
    "plt.title('KMeans: Inertia and Silhouette Score by Number of Clusters')\n",
    "plt.grid(True)\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Print both metrics in tabular form\n",
    "print(\"k\\tInertia\\t\\tSilhouette Score\")\n",
    "for i, k in enumerate(k_range):\n",
    "    print(f\"{k}\\t{inertias[i]:.2f}\\t\\t{silhouette_scores[i]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6141fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commit to clusters\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "kmeans_4 = KMeans(n_clusters=13, random_state=13)\n",
    "clusters4 = kmeans_4.fit_predict(factor_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b26eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clusters\n",
    "\n",
    "# Convert factor scores to a DataFrame if it's not already\n",
    "factor_df = pd.DataFrame(factor_scores, columns=[f'Factor {i+1}' for i in range(factor_scores.shape[1])])\n",
    "factor_df['Cluster'] = clusters4  # Add KMeans cluster labels\n",
    "\n",
    "# Plot first two factors\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.scatterplot(\n",
    "    x='Factor 1', y='Factor 2',\n",
    "    hue='Cluster',\n",
    "    palette='tab10',\n",
    "    data=factor_df,\n",
    "    alpha=0.7,\n",
    "    s=40\n",
    ")\n",
    "plt.title(\"KMeans Clusters on First Two Factor Scores\")\n",
    "plt.xlabel(\"Factor 1\")\n",
    "plt.ylabel(\"Factor 2\")\n",
    "plt.legend(title='Cluster')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Visualize with colors\n",
    "# sns.scatterplot(x=umap_embedding[:, 0], y=umap_embedding[:, 1], hue=clusters5, palette=\"viridis\", s=30)\n",
    "# plt.title(\"UMAP of Factor Scores with Clusters\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bdd53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parada_num_filtered.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d287f023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "# Step 1: Attach cluster labels to parada dataframe, ensuring alignment\n",
    "parada_t =  parada_num_concise.join(parada['lyrics'], how='inner')\n",
    "#parada_t['lyrics'] = parada['lyrics'].dropna()\n",
    "#parada_t = parada.reset_index(drop=True)  # Ensure matching index\n",
    "parada_t['Cluster'] = clusters4        # Add clusters to main dataframe\n",
    "\n",
    "# Step 2: Initialize dictionary to store topic models per cluster\n",
    "cluster_topic_models = {}\n",
    "\n",
    "# Step 3: Loop through clusters and apply BERTopic\n",
    "for cluster_id in sorted(parada_t['Cluster'].unique()):\n",
    "    cluster_lyrics = parada_t[parada_t['Cluster'] == cluster_id]['lyrics'].dropna().astype(str).tolist()\n",
    "\n",
    "    if len(cluster_lyrics) > 10:  # Skip clusters with very few entries\n",
    "        print(f\"\\nFitting topic model for Cluster {cluster_id} with {len(cluster_lyrics)} documents...\")\n",
    "        topic_model = BERTopic()\n",
    "        topics, probs = topic_model.fit_transform(cluster_lyrics)\n",
    "        cluster_topic_models[cluster_id] = topic_model\n",
    "\n",
    "        # Optionally print top topic\n",
    "        print(topic_model.get_topic_info().head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac277bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neighbors import NearestNeighbors\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "\n",
    "# neigh = NearestNeighbors(n_neighbors=5)\n",
    "# nbrs = neigh.fit(parada_num_concise.drop(columns=['dbscan_cluster'], errors='ignore'))\n",
    "# distances, indices = nbrs.kneighbors(parada_num_concise.drop(columns=['dbscan_cluster'], errors='ignore'))\n",
    "\n",
    "# distances = np.sort(distances[:, 4])  # 4th NN\n",
    "# plt.plot(distances)\n",
    "# plt.axhline(y=0.5, color='r', linestyle='--')  # current eps\n",
    "# plt.title(\"K-distance Plot (4th NN)\")\n",
    "# plt.xlabel(\"Data Points sorted by distance\")\n",
    "# plt.ylabel(\"4th Nearest Neighbor Distance\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export as df:\n",
    "\n",
    "# # Step 1: Filter out rows with missing values in keeper columns\n",
    "# parada_clusters = parada[parada[['Cognition', 'Linguistic', 'swear', 'emo_pos']].notna().all(axis=1)].copy()\n",
    "\n",
    "# # Step 2: Add the cluster labels\n",
    "# parada_clusters['cluster'] = clusters5\n",
    "\n",
    "# # Step 3: Create the factor scores DataFrame (factor_scores should be aligned with the same filtered rows)\n",
    "# factor_df = pd.DataFrame(\n",
    "#     factor_scores,\n",
    "#     columns=[f'Factor {i+1}' for i in range(factor_scores.shape[1])],\n",
    "#     index=parada_clusters.index  # <- Use parada_clusters index here for perfect alignment\n",
    "# )\n",
    "\n",
    "# # Step 4: Concatenate factor scores into parada_clusters\n",
    "# parada_clusters = pd.concat([parada_clusters, factor_df], axis=1)\n",
    "\n",
    "# # Step 5: Check the columns\n",
    "# print(parada_clusters.columns.tolist())\n",
    "\n",
    "# # Step 6: Load the CSV file and keep only the first two columns\n",
    "# songs_df = pd.read_csv('data/songs_expanded_25_04_01.csv', usecols=[0, 1])\n",
    "\n",
    "# # Step 7: Ensure index alignment — check and align if needed\n",
    "# # Option 1: If index matches parada_clusters\n",
    "# # parada_clusters = parada_clusters.merge(songs_df, left_index=True, right_index=True)\n",
    "\n",
    "# # Option 2: If there's a common key column (e.g., 'song_id' or 'title'), merge on that\n",
    "# # Replace 'key_column' with the actual shared column name\n",
    "# # parada_clusters = parada_clusters.merge(songs_df, on='key_column', how='left')\n",
    "\n",
    "# # Step 7 (recommended): Add the two columns directly, assuming order matches\n",
    "# parada_clusters = parada_clusters.reset_index(drop=True)  # Ensure matching index\n",
    "# songs_df = songs_df.reset_index(drop=True)\n",
    "\n",
    "# parada_clusters = pd.concat([songs_df, parada_clusters], axis=1)\n",
    "\n",
    "# # Step 8: Check final DataFrame\n",
    "# print(parada_clusters.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c43781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## UMAP\n",
    "# import umap\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Initialize UMAP\n",
    "# for n in [5, 15, 30, 50]:\n",
    "#     embedding = umap.UMAP(n_neighbors=n, random_state=13).fit_transform(parada_num_concise)\n",
    "#     plt.scatter(embedding[:, 0], embedding[:, 1], s=5)\n",
    "#     plt.title(f'n_neighbors={n}')\n",
    "#     plt.show()\n",
    "\n",
    "# # Fit and transform the factor scores\n",
    "# # umap_embedding = reducer.fit_transform(factor_scores)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import umap\n",
    "# from sklearn.cluster import KMeans\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Set up values to test\n",
    "# neighbors_range = [5,  50, 75]\n",
    "# silhouette_scores = []\n",
    "\n",
    "# for n in neighbors_range:\n",
    "#     # Step 1: UMAP projection\n",
    "#     reducer = umap.UMAP(n_neighbors=n, n_components=2, random_state=13)\n",
    "#     embedding = reducer.fit_transform(factor_scores)  # or your high-dim data\n",
    "\n",
    "#     # Step 2: Clustering (adjust k as needed)\n",
    "#     kmeans = KMeans(n_clusters=5, random_state=13)\n",
    "#     labels = kmeans.fit_predict(embedding)\n",
    "\n",
    "#     # Step 3: Silhouette score\n",
    "#     score = silhouette_score(embedding, labels)\n",
    "#     silhouette_scores.append(score)\n",
    "\n",
    "# # Step 4: Plot the results\n",
    "# plt.figure(figsize=(8, 5))\n",
    "# plt.plot(neighbors_range, silhouette_scores, marker='o')\n",
    "# plt.title(\"Silhouette Score vs n_neighbors\")\n",
    "# plt.xlabel(\"UMAP n_neighbors\")\n",
    "# plt.ylabel(\"Silhouette Score\")\n",
    "# plt.grid(True)\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caf91bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP\n",
    "# factor_cols = [col for col in parada_clusters.columns if col.startswith('Factor')]\n",
    "\n",
    "# # Step 2: Generate UMAP embeddings\n",
    "# umap_model = UMAP(random_state=13)\n",
    "# umap_embedding = umap_model.fit_transform(parada_clusters[factor_cols])\n",
    "\n",
    "# # Step 3: Plot the scatterplot with hue by cluster\n",
    "# plt.figure(figsize=(12, 8))\n",
    "# sns.scatterplot(\n",
    "#     x=umap_embedding[:, 0],\n",
    "#     y=umap_embedding[:, 1],\n",
    "#     hue=parada_clusters['cluster'],\n",
    "#     palette='viridis',\n",
    "#     s=30,\n",
    "#     alpha=0.8,\n",
    "#     edgecolor='none'\n",
    "# )\n",
    "\n",
    "# # Step 4: Add artist labels (optional: limit to avoid clutter)\n",
    "# for i, artist in enumerate(parada_clusters.iloc[:10, 0]):  # \n",
    "#     plt.text(\n",
    "#         umap_embedding[i, 0],\n",
    "#         umap_embedding[i, 1],\n",
    "#         str(artist),\n",
    "#         fontsize=6,\n",
    "#         alpha=0.6\n",
    "#     )\n",
    "\n",
    "# plt.title(\"UMAP of Factor Scores with Clusters and Artist Labels\")\n",
    "# plt.xlabel(\"UMAP Dimension 1\")\n",
    "# plt.ylabel(\"UMAP Dimension 2\")\n",
    "# plt.legend(title='Cluster')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b2b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(parada_clusters.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8af9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parada_clusters.to_csv('data/songs_expandedc_25_05_12.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffc224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parada_clusters.describe(include='object')\n",
    "# print(parada_clusters.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e4949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parada_clusters = pd.read_csv('data/songs_expandedc_25_05_12.csv') \n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# keepers = [\"function\", \"Cognition\", \"cogproc\", \"Linguistic\", \"WC\",\"Comma\", \"swear\",\"Analytic\",\n",
    "# \"Social\", \"socrefs\", \"pronoun\", \"ppron\", \"space\", \"Perception\", \"article\"]\n",
    "# parada_clusters = pd.get_dummies(parada_clusters, columns=['emotion'], prefix='emotion')\n",
    "# parada_clusters = pd.get_dummies(parada_clusters, columns=['artist'], drop_first=False)\n",
    "# parada_clusters= parada_clusters.dropna()\n",
    "# drops = keepers + ['album','cluster', 'lyrics', 'song', 'Tone','lyrics-tokenized','emotion_scores_nrc', 'Factor 1', 'Factor 2']\n",
    "# #X = parada_clusters.drop(columns=drops)       # Features (e.g. factor scores, PCA, etc.)\n",
    "# X = parada_clusters[['surprise', 'joy', 'anger', 'neutral', 'sadness', 'fear', 'disgust', 'anger_nrc', 'fear_nrc', 'joy_nrc', 'sadness_nrc', 'surprise_nrc', 'disgust_nrc', 'trust_nrc', 'anticipation_nrc', 'negative_nrc', 'positive_nrc']]\n",
    "# y = parada_clusters['cluster']    \n",
    "\n",
    "\n",
    "# # Split data into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "\n",
    "# # Train model\n",
    "# clf = RandomForestClassifier(random_state=13)\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# # Predict on test set\n",
    "# y_pred = clf.predict(X_test)\n",
    "\n",
    "# # Evaluate performance\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ccdffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importances = clf.feature_importances_\n",
    "# features = X_train.columns\n",
    "\n",
    "# # Create a Series for easy visualization\n",
    "# feat_importance = pd.Series(importances, index=features).sort_values(ascending=False)\n",
    "\n",
    "# # Top 10 important features\n",
    "# feat_importance.head(10).plot(kind='barh')\n",
    "# plt.title(\"Top 10 Feature Importances\")\n",
    "# plt.gca().invert_yaxis()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tswift-mps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
